from __future__ import annotations
from typing import List, Optional, Tuple
from pathlib import Path
import numpy as np
import torch
import torch.nn as nn
from torch import Tensor
from torch_geometric.nn import global_mean_pool, global_max_pool

from ..ssl_graph_brep.models.ssl_brep import SSLBRepModule
from ..ssl_graph_brep.data_module.brep_dataset import BRepNPZDataset

# _embed(batch) Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°Ñ‚ÑŒ {'coedge': [NC,Dc], 'face': [NF,Df], 'edge': ...}

@torch.no_grad()
def extract_embeddings(
    model_ckpt: Path,
    data_dir: Path,
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
) -> Tuple[List[str], np.ndarray]:

    model = SSLBRepModule.load_from_checkpoint(model_ckpt).to(device).eval()
    ds = BRepNPZDataset(data_dir)
    ids: List[str] = []
    embs: List[np.ndarray] = []
    for i in range(len(ds)):
        data = ds.get(i)
        # Ð”Ð»Ñ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð³Ñ€Ð°Ñ„Ð° Ð½ÐµÑ‚ batch-Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð²: ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð½ÑƒÐ»ÐµÐ²Ñ‹Ðµ
        data = data.to(device)
        z = model._embed(data)  # {'coedge': [NC,Dc], 'face': [NF,Df], ...}
        co_batch = torch.zeros(z["coedge"].size(0), dtype=torch.long, device=device)
        fa_batch = torch.zeros(z["face"].size(0),   dtype=torch.long, device=device)
        g = _pool_model_embedding_balanced(z["coedge"], z["face"], co_batch, fa_batch)  # [1,D]
        embs.append(g.squeeze(0).cpu().numpy())
        ids.append(ds.files[i].stem)  # Ð¸Ð¼ÐµÐ½Ð° Ñ„Ð°Ð¹Ð»Ð¾Ð² ÐºÐ°Ðº Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ñ‹
    E = np.stack(embs).astype("float32")  # [N,D]
    # Ðš ÐºÐ¾ÑÐ¸Ð½ÑƒÑÑƒ: L2-Ð½Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²ÐºÐ° (Ð½Ð° Ð²ÑÑÐºÐ¸Ð¹ ÑÐ»ÑƒÑ‡Ð°Ð¹ ÐµÑ‰Ñ‘ Ñ€Ð°Ð·)
    E /= np.linalg.norm(E, axis=1, keepdims=True) + 1e-8
    return ids, E


def search_by_name(
    ids: List[str],
    E: np.ndarray,
    query_name: str,
    include_self: bool = True,
    max_print: Optional[int] = 50,
) -> List[Tuple[int, float, str]]:
    """
    ÐÐ°Ñ…Ð¾Ð´Ð¸Ñ‚ Ð¿Ð¾ Ð¸Ð¼ÐµÐ½Ð¸/Ð¿Ð¾Ð´ÑÑ‚Ñ€Ð¾ÐºÐµ Ð²ÑÐµ ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ñ Ð¸ Ð¿ÐµÑ‡Ð°Ñ‚Ð°ÐµÑ‚ Ñ€Ð°Ð½Ð¶Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¿Ð¾ ÐºÐ¾ÑÐ¸Ð½ÑƒÑÑƒ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ñ.
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ ÑÑ„Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº (ÐµÑÐ»Ð¸ Ð¼Ð°Ñ‚Ñ‡ÐµÐ¹ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾).
    """
    hits = _find_query_indices(ids, query_name)
    if len(hits) == 0:
        print(f"Ð—Ð°Ð¿Ñ€Ð¾Ñ '{query_name}': ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾")
        return []
    last_result: List[Tuple[int, float, str]] = []
    for idx in hits:
        print(f"\nÐ—Ð°Ð¿Ñ€Ð¾Ñ: {ids[idx]}")
        ranked = _rank_all_by_query(ids, E, idx, include_self=include_self)
        last_result = ranked
        # Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚: Ð½Ð¾Ð¼ÐµÑ€, %, Ð¸Ð¼Ñ
        for rank, sim, name in (ranked if max_print is None else ranked[:max_print]):
            pct = round(100.0 * sim, 2)
            print(f"{rank:4d}. {pct:6.2f}%  {name}")
    return last_result

def topk_similar(E: np.ndarray, k: int = 10, include_self: bool = True) -> np.ndarray:
    S = _cosine_similarity_matrix(E)  # ÐºÐ¾ÑÐ¸Ð½ÑƒÑ Ð¿Ñ€Ð¸ L2-Ð½Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²ÐºÐµ
    if not include_self:
        np.fill_diagonal(S, -1.0)  # Ð¸ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ self Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ñ€Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð·Ðµ ÑÐ¾ÑÐµÐ´ÐµÐ¹
    topk_idx = np.argpartition(-S, kth=range(k), axis=1)[:, :k] # type: ignore
    row_sort = np.argsort(-S[np.arange(S.shape[0])[:, None], topk_idx], axis=1) # type: ignore
    return topk_idx[np.arange(S.shape[0])[:, None], row_sort]

def _pool_model_embedding_balanced(
    z_coedge: Tensor, z_face: Tensor, 
    co_batch: Tensor, fa_batch: Tensor
) -> Tensor:
    """
    Ð¡Ð±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ñ Ñ Ð¼ÐµÐ½ÑŒÑˆÐµÐ¹ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒÑŽ
    """
    # Coedge pooling
    zc_mean = global_mean_pool(z_coedge, co_batch)  # [B, hidden]
    zc_max = global_max_pool(z_coedge, co_batch)    # [B, hidden]
    
    # Face pooling  
    zf_mean = global_mean_pool(z_face, fa_batch)    # [B, hidden]
    
    # Ð¡Ð¶Ð°Ñ‚Ð¸Ðµ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚Ð¸ Ñ‡ÐµÑ€ÐµÐ· Ð¿Ñ€Ð¾ÐµÐºÑ†Ð¸Ð¸
    target_dim = 256  # ÐÐ°Ð¼Ð½Ð¾Ð³Ð¾ Ð¼ÐµÐ½ÑŒÑˆÐµ Ñ‡ÐµÐ¼ 1536!
    
    coedge_combined = 0.7 * zc_mean + 0.3 * zc_max  # [B, hidden]
    
    # ÐŸÑ€Ð¾ÐµÐºÑ†Ð¸Ð¸ Ð² Ð¼ÐµÐ½ÑŒÑˆÑƒÑŽ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ
    coedge_proj = nn.functional.linear(coedge_combined, 
                                     torch.randn(target_dim//2, coedge_combined.size(-1)))
    face_proj = nn.functional.linear(zf_mean,
                                   torch.randn(target_dim//2, zf_mean.size(-1)))
    
    # Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ
    z = torch.cat([coedge_proj, face_proj], dim=1)  # [B, target_dim]
    z = torch.nn.functional.normalize(z, dim=-1)
    
    return z  # [B, D]

def _cosine_similarity_matrix(E: np.ndarray) -> np.ndarray:
    """
    ÐŸÑ€ÐµÐ´Ð¿Ð¾Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ, Ñ‡Ñ‚Ð¾ E ÑƒÐ¶Ðµ L2-Ð½Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð¿Ð¾ ÑÑ‚Ñ€Ð¾ÐºÐ°Ð¼. Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ S=E@E^T.
    """
    return E @ E.T

def _find_query_indices(ids: List[str], query: str, case_insensitive: bool = True) -> List[int]:
    """
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð² Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, Ñ‡ÑŒÐ¸ Ð¸Ð¼ÐµÐ½Ð° ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ñ‚ query (Ð¿Ð¾Ð´ÑÑ‚Ñ€Ð¾ÐºÐ°).
    """
    q = query.lower() if case_insensitive else query
    hits = []
    for i, name in enumerate(ids):
        s = name.lower() if case_insensitive else name
        if q in s:
            hits.append(i)
    return hits

def _euclidean_similarity_matrix(E: np.ndarray, method: str = 'exp') -> np.ndarray:
    """
    ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ð° ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð° Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÐµÐ²ÐºÐ»Ð¸Ð´Ð¾Ð²Ð° Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ñ.
    method: 'exp' (ÑÐºÑÐ¿Ð¾Ð½ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð·Ð°Ñ‚ÑƒÑ…Ð°Ð½Ð¸Ðµ) Ð¸Ð»Ð¸ 'inverse' (Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ðµ Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ðµ)
    """
    n = E.shape[0]
    
    # Ð’ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ Ð²ÑÐµÑ… Ð¿Ð¾Ð¿Ð°Ñ€Ð½Ñ‹Ñ… Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ð¹
    # Broadcasting: E[None,:,:] - E[:,None,:] -> [n,n,d]
    diff = E[None, :, :] - E[:, None, :]  # [n, n, d]
    distances = np.linalg.norm(diff, axis=2)  # [n, n]
    
    if method == 'exp':
        # ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð°Ñ ÑÐ¸Ð³Ð¼Ð° Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¼ÐµÐ´Ð¸Ð°Ð½Ð½Ð¾Ð³Ð¾ Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ñ
        sigma = np.median(distances[distances > 0])
        similarity = np.exp(-distances / sigma)
    elif method == 'inverse':
        similarity = 1 / (1 + distances)
    else:
        raise ValueError("method Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ 'exp' Ð¸Ð»Ð¸ 'inverse'")
    
    return similarity

# Ð—Ð°Ð¼ÐµÐ½Ð¸Ñ‚Ðµ Ð² Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑÑ… Ð¿Ð¾Ð¸ÑÐºÐ°:
def _rank_all_by_query(ids, E, query_idx, include_self=True):
    # Ð¡Ñ‚Ð°Ñ€Ñ‹Ð¹ ÑÐ¿Ð¾ÑÐ¾Ð±:
    # s = (E[query_idx:query_idx+1] @ E.T).ravel()
    
    # ÐÐ¾Ð²Ñ‹Ð¹ ÑÐ¿Ð¾ÑÐ¾Ð±:
    S = _euclidean_similarity_matrix(E)
    s = S[query_idx]
    
    if not include_self:
        s[query_idx] = -1.0
    order = np.argsort(-s)
    
    out = []
    for rank, j in enumerate(order, start=1):
        sim = float(s[j])
        out.append((rank, sim, ids[j]))
    return out

def analyze_embedding_distribution(ids: List[str], E: np.ndarray):
    """ÐÐ½Ð°Ð»Ð¸Ð· Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ ÑÑ…Ð¾Ð´ÑÑ‚Ð² Ð´Ð»Ñ Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ¸"""
    
    print("ðŸ” Ð”Ð˜ÐÐ“ÐÐžÐ¡Ð¢Ð˜ÐšÐ ÐšÐÐ§Ð•Ð¡Ð¢Ð’Ð Ð­ÐœÐ‘Ð•Ð”Ð”Ð˜ÐÐ“ÐžÐ’")
    print("="*50)
    
    # ÐšÐ¾ÑÐ¸Ð½ÑƒÑÐ½Ð¾Ðµ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð¾ (Ñ‚ÐµÐºÑƒÑ‰ÐµÐµ)
    S_cos = E @ E.T
    np.fill_diagonal(S_cos, 0)
    
    # Ð•Ð²ÐºÐ»Ð¸Ð´Ð¾Ð²Ð¾ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð¾  
    S_eucl = _euclidean_similarity_matrix(E)
    np.fill_diagonal(S_eucl, 0)
    
    print(f"\nðŸ“Š ÐšÐžÐ¡Ð˜ÐÐ£Ð¡ÐÐžÐ• Ð¡Ð¥ÐžÐ”Ð¡Ð¢Ð’Ðž:")
    print(f"   Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ: {S_cos.mean():.4f}")
    print(f"   ÐœÐµÐ´Ð¸Ð°Ð½Ð°: {np.median(S_cos):.4f}")  
    print(f"   Ð¡Ñ‚Ð´.Ð¾Ñ‚ÐºÐ»: {S_cos.std():.4f}")
    print(f"   Ð”Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½: [{S_cos.min():.4f}, {S_cos.max():.4f}]")
    
    print(f"\nðŸ“ Ð•Ð’ÐšÐ›Ð˜Ð”ÐžÐ’Ðž Ð¡Ð¥ÐžÐ”Ð¡Ð¢Ð’Ðž:")
    print(f"   Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ: {S_eucl.mean():.4f}")
    print(f"   ÐœÐµÐ´Ð¸Ð°Ð½Ð°: {np.median(S_eucl):.4f}")
    print(f"   Ð¡Ñ‚Ð´.Ð¾Ñ‚ÐºÐ»: {S_eucl.std():.4f}")
    print(f"   Ð”Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½: [{S_eucl.min():.4f}, {S_eucl.max():.4f}]")
    
    # ÐÐ½Ð°Ð»Ð¸Ð· Ð²Ñ‹ÑÐ¾ÐºÐ¾ÑÑ…Ð¾Ð´Ð½Ñ‹Ñ… Ð¿Ð°Ñ€
    high_cos = np.sum(S_cos > 0.98) / 2  # Ð”ÐµÐ»Ð¸Ð¼ Ð½Ð° 2 (ÑÐ¸Ð¼Ð¼ÐµÑ‚Ñ€Ð¸Ñ‡Ð½Ð°Ñ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð°)
    high_eucl = np.sum(S_eucl > 0.90) / 2
    total_pairs = len(ids) * (len(ids) - 1) / 2
    
    print(f"\nâš ï¸  ÐšÐžÐÐ¦Ð•ÐÐ¢Ð ÐÐ¦Ð˜Ð¯ Ð’Ð«Ð¡ÐžÐšÐžÐ“Ðž Ð¡Ð¥ÐžÐ”Ð¡Ð¢Ð’Ð:")
    print(f"   ÐšÐ¾ÑÐ¸Ð½ÑƒÑ >0.98: {high_cos}/{total_pairs:.0f} ({100*high_cos/total_pairs:.1f}%)")
    print(f"   Ð•Ð²ÐºÐ»Ð¸Ð´ >0.90: {high_eucl}/{total_pairs:.0f} ({100*high_eucl/total_pairs:.1f}%)")


