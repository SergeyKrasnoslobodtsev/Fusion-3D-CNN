import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import json

from .models.self_supervised_ensemble import SelfSupervisedFusionModel
from .data_module.enhanced_data_loader import EnhancedFusionDataModule

class SearchSimulator:
    """–°–∏–º—É–ª—è—Ç–æ—Ä –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö 3D –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(
        self, 
        model: SelfSupervisedFusionModel, 
        data_module: EnhancedFusionDataModule,
        device: torch.device
    ):
        self.model = model
        self.data_module = data_module
        self.device = device
        
        # –ö–µ—à –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embeddings_cache: Optional[Dict[str, torch.Tensor]] = None
        self.item_ids: Optional[List[str]] = None
        
    def build_index(self, use_train: bool = True, use_val: bool = True) -> None:
        """–°—Ç—Ä–æ–∏—Ç –∏–Ω–¥–µ–∫—Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        
        print("üîÑ –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        
        embeddings = []
        item_ids = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ train –∏ val
        loaders = []
        if use_train:
            loaders.append(("train", self.data_module.train_dataloader()))
        if use_val:
            loaders.append(("val", self.data_module.val_dataloader()))
        
        self.model.eval()
        with torch.no_grad():
            for split_name, loader in loaders:
                print(f"  –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {split_name} split...")
                
                for batch in tqdm(loader, desc=f"Extracting {split_name} embeddings"):
                    # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ device
                    batch_device = {
                        k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                        for k, v in batch.items()
                    }
                    
                    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                    outputs = self.model(batch_device)
                    batch_embeddings = outputs['projections'].cpu()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏
                    
                    embeddings.append(batch_embeddings)
                    item_ids.extend(batch['item_id'])
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        all_embeddings = torch.cat(embeddings, dim=0)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
        self.embeddings_cache = {
            item_id: embedding for item_id, embedding in zip(item_ids, all_embeddings)
        }
        self.item_ids = item_ids
        
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω: {len(self.item_ids)} –º–æ–¥–µ–ª–µ–π, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {all_embeddings.shape[1]}")
    
    def search_similar(
        self, 
        query_item_id: str, 
        top_k: int = 10,
        exclude_self: bool = True
    ) -> List[Tuple[str, float]]:
        """
        –ü–æ–∏—Å–∫ —Ç–æ–ø-K –ø–æ—Ö–æ–∂–∏—Ö –º–æ–¥–µ–ª–µ–π
        
        Args:
            query_item_id: ID –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            exclude_self: –ò—Å–∫–ª—é—á–∏—Ç—å —Å–∞–º—É –º–æ–¥–µ–ª—å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            List[(item_id, similarity_score)] –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏
        """
        
        if self.embeddings_cache is None:
            raise RuntimeError("–°–Ω–∞—á–∞–ª–∞ –ø–æ—Å—Ç—Ä–æ–π—Ç–µ –∏–Ω–¥–µ–∫—Å —Å –ø–æ–º–æ—â—å—é build_index()")
        
        if query_item_id not in self.embeddings_cache:
            raise KeyError(f"–ú–æ–¥–µ–ª—å '{query_item_id}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ")
        
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.embeddings_cache[query_item_id]
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Å–æ –≤—Å–µ–º–∏ –º–æ–¥–µ–ª—è–º–∏
        similarities = []
        for item_id, embedding in self.embeddings_cache.items():
            if exclude_self and item_id == query_item_id:
                continue
                
            # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (—ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã)
            similarity = torch.dot(query_embedding, embedding).item()
            similarities.append((item_id, similarity))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        return similarities[:top_k]
    
    def search_by_embedding(
        self, 
        query_embedding: torch.Tensor, 
        top_k: int = 10
    ) -> List[Tuple[str, float]]:
        """–ü–æ–∏—Å–∫ –ø–æ –≥–æ—Ç–æ–≤–æ–º—É —ç–º–±–µ–¥–¥–∏–Ω–≥—É"""
        
        if self.embeddings_cache is None:
            raise RuntimeError("–°–Ω–∞—á–∞–ª–∞ –ø–æ—Å—Ç—Ä–æ–π—Ç–µ –∏–Ω–¥–µ–∫—Å —Å –ø–æ–º–æ—â—å—é build_index()")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
        query_embedding = F.normalize(query_embedding, p=2, dim=0)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = []
        for item_id, embedding in self.embeddings_cache.items():
            similarity = torch.dot(query_embedding, embedding).item()
            similarities.append((item_id, similarity))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]
    
    def evaluate_retrieval(
        self, 
        test_queries: Optional[List[str]] = None,
        top_k: int = 10
    ) -> Dict[str, float]:
        """
        –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å ground truth)
        """
        
        if test_queries is None:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            test_queries = np.random.choice(self.item_ids, size=min(50, len(self.item_ids)), replace=False).tolist() # type: ignore
        
        print(f"üß™ –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ –Ω–∞ {len(test_queries)} –∑–∞–ø—Ä–æ—Å–∞—Ö...") # type: ignore 
        
        # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞: —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        diversity_scores = []
        avg_similarities = []
        
        for query_id in tqdm(test_queries):
            results = self.search_similar(query_id, top_k=top_k)
            
            if not results:
                continue
                
            # –°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å
            similarities = [score for _, score in results]
            avg_similarities.append(np.mean(similarities))
            
            # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ—Å—Ç—å (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏)
            diversity_scores.append(np.std(similarities))
        
        metrics = {
            'avg_similarity': np.mean(avg_similarities),
            'avg_diversity': np.mean(diversity_scores),
            'num_queries': len(test_queries) # type: ignore
        }
        
        return metrics
    
    def visualize_search_results(
        self, 
        query_item_id: str, 
        top_k: int = 10,
        save_path: Optional[Path] = None
    ) -> None:
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        
        results = self.search_similar(query_item_id, top_k=top_k)
        
        # –°–æ–∑–¥–∞—ë–º DataFrame –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        df = pd.DataFrame(results, columns=['item_id', 'similarity'])
        df['rank'] = range(1, len(df) + 1)
        
        # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫
        plt.figure(figsize=(12, 6))
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏
        plt.subplot(1, 2, 1)
        sns.barplot(data=df, x='rank', y='similarity', palette='viridis')
        plt.title(f'–¢–æ–ø-{top_k} –ø–æ—Ö–æ–∂–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è {query_item_id}')
        plt.xlabel('–†–∞–Ω–≥')
        plt.ylabel('–ö–æ—Å–∏–Ω—É—Å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å')
        plt.xticks(rotation=45)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        plt.subplot(1, 2, 2)
        plt.axis('tight')
        plt.axis('off')
        table_data = df[['rank', 'item_id', 'similarity']].round(3)
        table = plt.table(
            cellText=table_data.values,
            colLabels=['–†–∞–Ω–≥', 'ID –º–æ–¥–µ–ª–∏', '–°—Ö–æ–∂–µ—Å—Ç—å'],
            cellLoc='center',
            loc='center'
        )
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1.2, 1.5)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {save_path}")
        
        plt.show()
    
    def save_index(self, path: Path) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω–¥–µ–∫—Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if self.embeddings_cache is None:
            raise RuntimeError("–ò–Ω–¥–µ–∫—Å –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω")
            
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        embeddings_np = {
            item_id: embedding.numpy() 
            for item_id, embedding in self.embeddings_cache.items()
        }
        
        index_data = {
            'embeddings': embeddings_np,
            'item_ids': self.item_ids,
            'embedding_dim': list(embeddings_np.values())[0].shape[0]
        }
        
        np.savez_compressed(path, **index_data)
        print(f"üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {path}")
    
    def load_index(self, path: Path) -> None:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        data = np.load(path, allow_pickle=True)
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings_np = data['embeddings'].item()
        self.embeddings_cache = {
            item_id: torch.from_numpy(embedding) 
            for item_id, embedding in embeddings_np.items()
        }
        self.item_ids = data['item_ids'].tolist()

        print(f"üìÇ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω: {len(self.item_ids)} –º–æ–¥–µ–ª–µ–π") # type: ignore